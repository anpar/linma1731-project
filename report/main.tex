\documentclass[english, DIV=13]{scrartcl}

% Packages
\input{lib.tex}
\usepackage{placeins}
\usepackage{pgfplots}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{empheq}
\usepackage{hyperref}
\usepackage{todonotes}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\renewcommand{\vec}[1]{\mathbf{#1}}
\bibliographystyle{ieeetr}

\title{LINMA1731 - Project}
\author{Antoine Paris\and Mathieu Xhonneux}
\date{\today}

\begin{document}
\maketitle

\section{Discrete-time version of the Lorenz system}
Using first-order forward finite difference, the Lorenz dynamical system can be written as
\[ \vec{x}_{k+1} = F(\vec{x}_k) + \Gamma\vec{u}_k \]
where the function $F$ is given by
\begin{equation}
    \begin{cases}
        F_1(\vec{x}_k) &= ay_k\delta t + (1-a\delta t)x_k \\
        F_2(\vec{x}_k) &= x_k(r-z_k)\delta t + (1-\delta t)y_k \\
        F_3(\vec{x}_k) &= x_ky_k\delta t + (1-b\delta t)z_k
    \end{cases}.
    \label{eq:F}
\end{equation}

\section{Discrete-time system simulation and observation of the system}
A sample trajectory resulting from a 50 seconds simulation is given in figure~
\ref{fig:q2-3d-trajectory}. As expected for this set of parameters, the trajectory
is chaotic and shows the typical ``figure eigth'' form. 
The trajectory of the first coordinates and the corresponding noisy measurements
with $\sigma^2_m = 1$ are represented in figure~\ref{fig:q2-mes-vs-real}.
\begin{figure}[hb]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/q2-3d-trajectory}
        \caption{Realization of a 50 seconds simulated trajectory. The initial
        position is indicated by the green dot.}
        \label{fig:q2-3d-trajectory}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/q2-mes-vs-real}
        \caption{Trajectory of the first coordinates and corresponding noisy
        measurements with $\sigma^2_m = 1$.}
        \label{fig:q2-mes-vs-real}
    \end{subfigure}

    \caption{Simulations and measurements.}
\end{figure}

\FloatBarrier

\section{Sequential Monte Carlo}
The results of this section were obtained with a Python implementation of the Classical
Sequential Monte Carlo (CSMC) algorithm described in~\cite{anuj}. The only difference
in our case is that the sampling period $\Delta t$ and the process time step $\delta t$
differs. More specifically, $\Delta t/\delta t = L$ where $L$ is an integer. Because
of this, the prediction step in CSMS is composed of $L$ successive applications of the
function $F$ (and the addition of the noise on the dynamics) instead of just one as
in~\cite{anuj}.

To obtain a sufficient level of particles diversity, the variance of the noise on the
dynamics has finally be fixed to $\sigma^2_u = 0.01$. The effect of this parameter on
CSMC is discussed in section~\ref{sec:exp}.

\subsection{Particles distribution}
\label{sec:particles-distri}
Histograms of the samples distribution in $x_k$ and $y_t$ for $t = 5, 10, 15$ are
given in figures~\ref{fig:q3-hist-x-100} and \ref{fig:q3-hist-y-100} respectively
(after resampling). These two figures show that the particles tend to be normally
distributed around the real value. This is even more true when the number of particles
increases (with 1000 particles for example).

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/hist-x-100}
        \caption{Histogram of samples distributions in $x_k$.}
        \label{fig:q3-hist-x-100}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/hist-y-100}
        \caption{Histogram of samples distributions in $y_k$.}
        \label{fig:q3-hist-y-100}
    \end{subfigure}
    \caption{Histogram of samples distributions  at time $t = 0, 5, 15$
    (from left to right) for 100 particles. The dashed green lines represent the real
    values.}
    \label{fig:q3-hist}
\end{figure}

\subsection{Particles resampling}
Particles importance resampling is illustrated in figures~\ref{fig:particle-5-100}
and~\ref{fig:particle-15-100} for 100 particles. The objective of this importance
resampling is to choose the most probable particles with respect to the observed
value. One can visually check from the histograms of the preceding section (which
contains the real value as a dashed green line) that the resampled particles indeed tend
to concentrate around the real value.

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/particles-5-100}
        \caption{At $t=5$.}
        \label{fig:particle-5-100}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/particles-15-100}
        \caption{At $t=15$.}
        \label{fig:particle-15-100}
    \end{subfigure}
    \caption{Particles before and after resampling for 100 particles.
    Transparence level of red crosses indicates if particles have been chosen
    multiples times (the less transparent, the more a particle has been chosen).}
\end{figure}

\subsection{Estimation error}
Estimation errors, measured as 
$\norm{\mathbf{x}_t^{\text{real}} - \sum_{i=1}^n w_t^i\tilde{\mathbf{x}}_t^i}_2$,
are given in figure~\ref{fig:q3-error-50}, \ref{fig:q3-error-100} and
\ref{fig:q3-error-1000} for $n = 50, 100, 1000$ particles repectively.
From these figures, it can be observed that the estimation error decreases with the
number of particles. This is linked to the observations made previously about the
particles distribution. The higher the number of particles, the more this distribution
is close to a normal centered on the real value. Peak of large errors also seems to
rarify with higher number of particles. This can be explained as follows: a higher
number of particles means a larger diversity across the particles and a larger diversity
means a better ability to cope with large fluctuations in the dynamics of the system.

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/error-50}
        \caption{For $n=50$ particles.} 
        \label{fig:q3-error-50}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/error-100}
        \caption{For $n=100$ particles.} 
        \label{fig:q3-error-100}
    \end{subfigure}\\
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/error-1000}
        \caption{For $n=1000$ particles.} 
        \label{fig:q3-error-1000}
    \end{subfigure}
    \caption{Error as a function of time for different numbers of particles. The blue
    lines correspond to
    $\norm{\mathbf{x}_t^{\text{real}} - \sum_{i=1}^n w_t^i\tilde{\mathbf{x}}_t^i}_2$.
    The mean error on the first coordinate has also been plotted.}
\end{figure}

\subsection{Experiments}
\label{sec:exp}
\paragraph{Effect of $\sigma^2_u$}
The noise on the dynamics $\sigma^2_u$ highly impacts the diversity of the particles set.
A too small $\sigma^2_u$ causes the particles set to quickly degenerate, that is to
contain only a few number of very close particles. When this happens, the algorithm is
no longer able to efficiently estimate the trajectory and the error skyrockets. Larger
values of $\sigma^2_u$ add uncertainty on the dynamics of the system. This obviously
limits the accuracy of the prediction made by the algorithm but helps a lot to keep
a diverse set of particles.

\paragraph{Effect of $\sigma^2_0$}
The variance of the initial position $\sigma^2_0$ can be seen as how much
\textit{a priori} knowledge on the initial position the algorithm has.
Intuitively, if $\sigma^2_0$ is large, the algorithm will generate a diverse initial set
of particles. If $\sigma^2_0$ is small, the algorithm will generate an initial set more
concentrated on the mean initial position. As a consequence, a large $\sigma^2_0$ might
lead to a large initial estimation error before the particle set concentrates more. This
was indeed observed on a simulation with 100 particles and a very large $\sigma^2_0$ where
peak errors up to a value of 10 appear initialy. However, the situation then quickly goes
back the normal. This is illustrated in figure~\ref{fig:error-100-s0-high}.

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/error-100-s0-high}
        \caption{Effect of a large $\sigma^2_0$.}
        \label{fig:error-100-s0-high}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/error-100-sm-low}
        \caption{Effect of a small $\sigma^2_m$.}
        \label{fig:error-100-sm-low}
    \end{subfigure}
    \caption{Effect of $\sigma^2_0$ and $\sigma^2_m$ on the error on SMSC with
    100 particles.}
\end{figure}

\paragraph{Effect of $\sigma^2_m$}
The variance of the measurement noise $\sigma^2_m$ mainly impacts the estimation error on
the first coordinate. Figure~\ref{fig:error-100-sm-low} shows the error curve for a very
low $\sigma^2_m$. The mean error on the first coordinate (dashed green line) is indeed
a lot smaller than in figure~\ref{fig:q3-error-1000} where $\sigma^2_m = 1$ for example.
On the other hand, the global error (i.e. $\norm{\mathbf{x}_t^{\text{real}} -
\sum_{i=1}^n w_t^i\tilde{\mathbf{x}}_t^i}_2$) does not vary much with $\sigma^2_m$.
Indeed, even if the mean error on $x$ decreases, the errors on $y$ and $z$ will not
necessarily decrease.

\paragraph{Effect of $\delta t$ and $\Delta t$}
The effect of the ratio of $\Delta t$ to $\delta t$ will now be discussed. Intuitively,
having $\Delta t/\delta t = 1$ seems to be ideal. Indeed, this reduces the uncertainty
on the predictions caused by the noise on the dynamics (because we only have to apply
the function $F$ and the addition of the noise $u$ once). As a consequence the mean
estimation error on the first coordinate decreases (and the global estimation error
thus also decreases slightly).
However, this might also accelerate the degeneracy of the particles set as more
resampling are needed. At the opposite, having a large $\Delta t/\delta t$ has the
effect of increasing the estimation error because more uncertainty due to the noise on
the dynamics accumulates in the predictions before the predictions gets resampled using
the observed value.

\paragraph{Effect of $\Gamma$}
To be fair in the comparison, we used $\Gamma$ matrices that conserve the effective
noise variance on each coordinates. For example,
\begin{equation*}
    \Gamma = \frac{1}{\sqrt{3}}
    \begin{pmatrix}
        1 & 1 & 1 \\
        1 & 1 & 1 \\
        1 & 1 & 1
    \end{pmatrix}
\end{equation*}
conserves the noise variance on each coordinates. Such a $\Gamma$ matrix imposes the
same noise on each coordinate. Having the same noise on each coordinate reduces a lot
the global estimation error which is around 1 with 100 particles (better than with
1000 particles and $\Gamma = I_3$).

\subsection{Results on the given dataset}
The trajectories for $x$, $y$ and $z$ coordinates are given in
figures~\ref{fig:x-trajectory-data}, \ref{fig:y-trajectory-data} and
\ref{fig:z-trajectory-data} respectively.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/x-trajectory-data}
        \caption{Trajectory of the $x$ coordinate.} 
        \label{fig:x-trajectory-data}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/y-trajectory-data}
        \caption{Trajectory of the $y$ coordinate.} 
        \label{fig:y-trajectory-data}
    \end{subfigure}\\
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/z-trajectory-data}
        \caption{Trajectory of the $z$ coordinate.} 
        \label{fig:z-trajectory-data}
    \end{subfigure}
    \caption{Estimated trajectories and particles positions at time $t=0, 5, 10$.
    CSMC has been run with 1000 particles.}
\end{figure}

\section{Implementation of an Extended Kalman Filter}
The Extended Kalman Filter amounts to applying a regular Kalman filter to the
linearization of the system. Our system can be linearized and written as follows
\begin{equation*}
    \begin{cases}
        \vec{x}_{k+1} = F_k\vec{x}_k + \Gamma\vec{u}_k \\
        m_k = x_k + w_k
    \end{cases}.
\end{equation*}
with $x_0 \sim\mathcal{N}(\vec{\mu}_0, \vec{P}_0 = \sigma^2_0)$, $u_k
\sim\mathcal{N}(\vec{0}, \vec{Q} = \vec{I_3}\sigma^2_u)$,
$w_k \sim\mathcal{N}(0, R = \sigma^2_m)$ and where $F_k$ is the Jacobian of the
function $F$ as defined in~\eqref{eq:F} evaluated at the current predicted state.
For each time step $k$, this Jacobian is given by
\begin{equation*}
    F_k = 
    \begin{pmatrix}
        1 - a \delta t & a \delta t & 0 \\
        (r - z_k) \delta t & 1 - \delta t & - x_k \delta t \\
        y_k \delta t & x_k \delta t & 1 - b \delta t.
    \end{pmatrix}
\end{equation*}
It is important not to confuse $F$, the function used in the discrete-time version of
the Lorenz system, and $F_k$ the Jacobian of $F$ evaluated at $\vec{x}_k$.
The Kalman filter equations can now be applied directly. The prediction step is written
as follows:
\[ \vec{\hat{x}}_{k|k-1} = F(\vec{x}_{k-1|k-1}) \]
\[ \vec{P}_{k|k-1} = \vec{F}_{k-1} \vec{P}_{k-1|k-1} \vec{F}_{k-1}^\mathsf{T} + 
\Gamma\vec{Q}\Gamma. \]
In our case, when the sampling period $\Delta t/\delta t = L$ with $L$ an integer, the
Jacobian has to be applied $L$ times (while re-evaluating it each time around the latest
obtained value) to obtain the prediction of the covariance.  
The Kalman gain, used in the update step is given by
\[ \vec{K}_k = \vec{P}_{k|k-1} \vec{H}^\mathsf{T} (\vec{H} ~ \vec{P}_{k|k-1}
\vec{H}^\mathsf{T} + R)^{-1} \]
where $\vec{H} = \left[1~0~0\right]$ because only the first coordinate is measured.
The updated estimation can then be computed as usual, that is
\[ \vec{\hat{x}}_{k|k} = \vec{\hat{x}}_{k|k-1} + \vec{K}_k (m_k  - \vec{H} ~
\vec{\hat{x}}_{k|k-1}) \]
\[ \vec{P}_{k|k} = \vec{P}_{k|k-1} - \vec{K}_k ~ \vec{H} ~ \vec{P}_{k|k-1}. \]
This was quite straightforwardly implemented in Python.
Figure~\ref{fig:ekf-results} shows side by side the estimation error for SMC and for EKF.
To make the comparison fair, the same parameters (variance of the noise on the dynamics,
variance of the measurement noise, etc) were used on the same measurements set for both
algorithm. Note that figure~\ref{fig:smc-error-1000} has been repeated to ease the
comparison but is identical to figure~\ref{fig:q3-error-1000}. This figure shows that
EKF is a bit less precise than SMC. However, the difference is quite small and is largely
compensated by the speed of EKF compared to SMC (especially when the latter uses a
high number of particles).

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/error-1000}
        \caption{Estimation error with SMC with 1000 particles.}
        \label{fig:smc-error-1000}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/ekf-error}
        \caption{Estimation error with EKF.} 
    \end{subfigure}
    \caption{Estimation errors comparison for EKF and SMC.}
    \label{fig:ekf-results}
\end{figure}

\section{Distributions comparison}
Figure~\ref{fig:space-cdf} shows the state space distributions for both CSMC and EKF
filtering. In both cases, the cumulative distributive functions have the
typical form of a Normal distribution. This was already observed in
section~\ref{sec:particles-distri} on the histograms for CSMS and was totally expected
for EKF by Gaussianity and linearity of the linearized system. 
Figure~\ref{fig:distrib-x} shows that the difference in CDF between EKF and CSMC is
negligible for the first coordinate of the trajectory. This is less true for the
second and third coordinates where the means can differ by up to 1 (see
figures~\ref{fig:distrib-y} and \ref{fig:distrib-z}). In term of variance, however,
the CDFs match pretty well for all three coordinates.

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/distrib-x}
        \caption{Space distribution for $x$ coordinate} 
        \label{fig:distrib-x}
    \end{subfigure}%
    ~
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/distrib-y}
        \caption{Space distribution for $y$ coordinate}
        \label{fig:distrib-y}
    \end{subfigure}%
    \\
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/distrib-z}
        \caption{Space distribution for $z$ coordinate} 
        \label{fig:distrib-z}
    \end{subfigure}%
    \caption{State space CDF for CSMC and EKF filtering at $t = \SI{5}{s}$.
    To obtain smooth curves, 10000 samples were drawned from a Normal distribution with
    the mean and the variance provided by EKF at $t = \SI{5}{s}$ and CSMC used 10000
    particles.}
    \label{fig:space-cdf}
\end{figure}

\bibliography{biblio}

\end{document}
